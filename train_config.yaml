# train_config.yaml

# Dataset configuration
dataset_path: "./data/shakespeare_char"
block_size: 256  # Context length

# Training setup
device: "cuda"  # or "cpu"
batch_size: 64
num_iterations: 100000
seed: 42

# Model architecture
n_embd: 768
n_layers: 12
n_heads: 12  # Should be n1 + n2 + n3
head_dim: 64  # n_embd must be divisible by n_heads*head_dim

# Attention geometry configuration
head_mode: "hyp"  # Options: euc, hyp, sph
n1: 4            # Number of Euclidean heads
n2: 4            # Number of Hyperbolic heads
n3: 4            # Number of Spherical heads
curvature: 0.1    # Initial curvature for hyperbolic heads
k_lr: 1e-4       # Learning rate for curvature parameters

# Optimization parameters
lr: 6e-4         # Base learning rate
end_lr: 1e-5     # Final learning rate after cooldown
cooldown_frac: 0.1  # Fraction of training for LR cooldown

# Logging and monitoring
train_loss_every: 100   # Log training loss every N steps
val_loss_every: 1000    # Calculate validation loss every N steps
generate_every: 2000    # Generate samples every N steps
gradient_log_every: 500 # Log gradient statistics every N steps

# Generation parameters
generation_temp: 0.8    # Temperature for sampling
top_k: 50               # Top-k filtering
max_gen_length: 500     # Max tokens to generate

# Precision and performance
use_amp: true           # Use automatic mixed precision
compile_model: true      # Compile model with torch.compile

# Regularization
dropout: 0.1            # Dropout probability
weight_decay: 0.1       # AdamW weight decay
grad_clip: 1.0          # Gradient clipping value